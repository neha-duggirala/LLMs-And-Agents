{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bfa82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, PromptTemplate\n",
    "from vector_store import youtube_url_to_documents\n",
    "from prompt import format_context_for_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb58211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2021b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b610168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessage(\"The user watches a video, and after sometime he wants to comeback to a particular point in the video discussing about some topic. You are a YouTube assistant that helps users find the timestamp in a video. When given a transcript and its timestamps, provide the timestamp(s) for the requested topic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39ca80c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [system_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12320eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import Field, field_validator\n",
    "import re\n",
    "\n",
    "class TimeStamp(BaseModel):\n",
    "    timestamp: List[str] = Field(description='This has a list of timestamps in MM:SS format like [\"20:53\", \"20:58\"]')\n",
    "\n",
    "    @field_validator(\"timestamp\")\n",
    "    def validate_timestamp(cls, v):\n",
    "        for ts in v:\n",
    "            if not re.match(r\"^\\d{1,2}:\\d{2}$\", ts):\n",
    "                raise ValueError(\"Timestamp must be in MM:SS format\")\n",
    "            minutes, seconds = map(int, ts.split(\":\"))\n",
    "            if not (0 <= minutes and 0 <= seconds < 60):\n",
    "                raise ValueError(\"Minutes must be >= 0 and seconds must be between 0 and 59\")\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70a2c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=TimeStamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d63638",
   "metadata": {},
   "source": [
    "# Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d04e285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"common loss function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a5cb9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_url_input = \"https://www.youtube.com/watch?v=FLkUOkeMd5M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a47a142a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com/watch?v=FLkUOkeMd5M'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_url_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "990e7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = youtube_url_to_documents(user_url_input)\n",
    "context = vector_store.similarity_search(\n",
    "    query=user_input,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "formatted_context = format_context_for_llm(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "165faaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'timestamp': '20:58', 'https://www.youtube.com/watch?v=zxQyTK8quyY': 'zxQyTK8quyY'}, page_content='parallel Computing and run fast'),\n",
       " Document(metadata={'timestamp': '20:58', 'https://www.youtube.com/watch?v=zxQyTK8quyY': 'zxQyTK8quyY'}, page_content='parallel Computing and run fast'),\n",
       " Document(metadata={'timestamp': '20:58', 'url': 'zxQyTK8quyY'}, page_content='parallel Computing and run fast'),\n",
       " Document(metadata={'timestamp': '20:53', 'https://www.youtube.com/watch?v=zxQyTK8quyY': 'zxQyTK8quyY'}, page_content='computation at the same time'),\n",
       " Document(metadata={'url': 'zxQyTK8quyY', 'timestamp': '20:53'}, page_content='computation at the same time')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "753835b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(template=\"{system_prompt}\\n use the followingt context and timestamps\\n{context}\\n Find the timestamps for the following user request\\n{input}\\n {format_instruction}\",\n",
    "               input_variables=[\"system_prompt\" ,\"context\", \"input\"],\n",
    "               partial_variables={'format_instruction':parser.get_format_instructions()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9c548c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"system_prompt\":system_prompt.content,\n",
    "        \"context\": formatted_context,\n",
    "        \"input\": user_input\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c0179f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='The user watches a video, and after sometime he wants to comeback to a particular point in the video discussing about some topic. You are a YouTube assistant that helps users find the timestamp in a video. When given a transcript and its timestamps, provide the timestamp(s) for the requested topic.\\n use the followingt context and timestamps\\nTimestamp: 20:58\\nContent: parallel Computing and run fast\\n\\nTimestamp: 20:58\\nContent: parallel Computing and run fast\\n\\nTimestamp: 20:58\\nContent: parallel Computing and run fast\\n\\nTimestamp: 20:53\\nContent: computation at the same time\\n\\nTimestamp: 20:53\\nContent: computation at the same time\\n\\n Find the timestamps for the following user request\\nparallel computation?\\n The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"timestamp\": {\"description\": \"This has a list of timestamps in MM:SS format like [\\\\\"20:53\\\\\", \\\\\"20:58\\\\\"]\", \"items\": {\"type\": \"string\"}, \"title\": \"Timestamp\", \"type\": \"array\"}}, \"required\": [\"timestamp\"]}\\n```')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63fdbe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bd310a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\"timestamp\": [\"20:58\", \"20:53\"]}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 330, 'total_tokens': 347, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BcBHhbbCc6qlPtGrvsLYstWhGsrfm', 'finish_reason': 'stop', 'logprobs': None}, id='run--29d159d1-d318-412a-bae0-88e76546166f-0', usage_metadata={'input_tokens': 330, 'output_tokens': 17, 'total_tokens': 347, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a238bc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStamp(timestamp=['20:58', '20:53'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.invoke(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e164d8",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1669a0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp=['17:02', '18:27', '18:50', '19:45', '19:01']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "# Define a lambda to retrieve context from the vector store\n",
    "def retrieve_context(inputs):\n",
    "    vector_store = youtube_url_to_documents(inputs[\"url\"])\n",
    "    context = vector_store.similarity_search(\n",
    "        query=inputs[\"query\"],\n",
    "        k=5\n",
    "    )\n",
    "    return {\"context\": format_context_for_llm(context)}\n",
    "\n",
    "# Define a lambda to format the prompt\n",
    "def build_prompt(inputs):\n",
    "    return template.invoke({\n",
    "        \"system_prompt\": system_prompt.content,\n",
    "        \"context\": inputs[\"context\"],\n",
    "        \"input\": inputs[\"query\"]\n",
    "    })\n",
    "\n",
    "# Parallel chain: retrieves context and builds prompt in parallel, then sends to LLM\n",
    "parallel_chain = (\n",
    "    RunnableParallel(\n",
    "        context=RunnableLambda(retrieve_context),\n",
    "        query=lambda x: x[\"query\"]\n",
    "    )\n",
    "    | RunnableLambda(lambda inputs: {\n",
    "        \"prompt\": build_prompt({\"context\": inputs[\"context\"][\"context\"], \"query\": inputs[\"query\"]})\n",
    "    })\n",
    "    | RunnableLambda(lambda inputs: llm.invoke(inputs[\"prompt\"]))\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Example usage:\n",
    "inputs = {\n",
    "    \"url\": user_url_input,\n",
    "    \"query\": user_input\n",
    "}\n",
    "result = parallel_chain.invoke(inputs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fd8d485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['17:02', '18:27', '18:50', '19:45', '19:01']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337cdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
